# Tasks: ç»Ÿä¸€ LLM API é…ç½®æ ¼å¼

**Input**: Design documents from `/specs/018-unified-llm-api/`  
**Prerequisites**: plan.md, spec.md, data-model.md, research.md, quickstart.md

**Organization**: Tasks are grouped by user story to enable independent implementation and testing.

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: é¡¹ç›®ä¾èµ–æ›´æ–°å’ŒåŸºç¡€é…ç½®

- [ ] T001 ç¡®è®¤ anthropic SDK å·²åœ¨ backend/pyproject.toml ä¸­å£°æ˜ä¸ºä¾èµ–
- [ ] T002 [P] æ›´æ–° .env.example æ·»åŠ æ–°ç¯å¢ƒå˜é‡ç¤ºä¾‹ `LLM_API_TYPE`, `LLM_API_KEY` ç­‰

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: ç»Ÿä¸€é…ç½®æ¨¡å‹ï¼Œæ‰€æœ‰ User Story éƒ½ä¾èµ–æ­¤é˜¶æ®µå®Œæˆ

**âš ï¸ CRITICAL**: æ­¤é˜¶æ®µå¿…é¡»å®Œæˆåæ‰èƒ½å¼€å§‹ User Story å®ç°

- [ ] T003 é‡æ„ backend/app/config.pyï¼šæ·»åŠ  `llm_api_type` å­—æ®µ (Literal["anthropic", "openai"])
- [ ] T004 é‡æ„ backend/app/config.pyï¼šä¿®æ”¹ `llm_api_base` é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œ`llm_model` é»˜è®¤å€¼ä¸º `claude-sonnet-4-5-20250929`
- [ ] T005 é‡æ„ backend/app/config.pyï¼šæ·»åŠ  `effective_api_key`ã€`effective_api_base`ã€`effective_model` è®¡ç®—å±æ€§
- [ ] T006 é‡æ„ backend/app/config.pyï¼šæ·»åŠ  `is_configured` ç»Ÿä¸€é…ç½®æ£€æŸ¥å±æ€§ï¼Œæ›¿ä»£ `is_llm_configured` å’Œ `is_agent_configured`
- [ ] T007 é‡æ„ backend/app/config.pyï¼šä¿ç•™å‘åå…¼å®¹åˆ«å (`agent_api_key`, `openai_api_key` ç­‰)

**Checkpoint**: é…ç½®æ¨¡å‹é‡æ„å®Œæˆï¼Œå¯ä»¥å¼€å§‹ User Story å®ç°

---

## Phase 3: User Story 1 - ä½¿ç”¨ Anthropic APIï¼ˆé»˜è®¤åœºæ™¯ï¼‰(Priority: P1) ğŸ¯ MVP

**Goal**: åº”ç”¨ä»£ç ç»Ÿä¸€ä½¿ç”¨ Anthropic Python Clientï¼Œé»˜è®¤ç›´è¿ Anthropic API

**Independent Test**: è®¾ç½® `LLM_API_KEY` ä¸ºæœ‰æ•ˆ Anthropic Keyï¼Œå¯åŠ¨æœåŠ¡ï¼ŒéªŒè¯ SQL ç”Ÿæˆå’Œ Agent æ¨¡å¼å‡æ­£å¸¸

### Implementation for User Story 1

- [ ] T008 [US1] é‡æ„ backend/app/services/llm_service.pyï¼šå°† OpenAI Client æ›¿æ¢ä¸º Anthropic Client
- [ ] T009 [US1] é‡æ„ backend/app/services/llm_service.pyï¼šä¿®æ”¹ `select_relevant_tables()` æ–¹æ³•ä½¿ç”¨ Anthropic messages API
- [ ] T010 [US1] é‡æ„ backend/app/services/llm_service.pyï¼šä¿®æ”¹ `generate_sql()` æ–¹æ³•ä½¿ç”¨ Anthropic messages API
- [ ] T011 [US1] é‡æ„ backend/app/services/llm_service.pyï¼šæ›´æ–° Prompt æ ¼å¼é€‚é… Anthropicï¼ˆsystem prompt æ”¾å…¥ system å‚æ•°ï¼‰
- [ ] T012 [US1] é‡æ„ backend/app/services/llm_service.pyï¼šä½¿ç”¨ç»Ÿä¸€é…ç½® (`settings.effective_api_key`, `settings.effective_api_base`, `settings.effective_model`)
- [ ] T013 [US1] é‡æ„ backend/app/services/agent_service.pyï¼šä½¿ç”¨ç»Ÿä¸€é…ç½®æ›¿ä»£ `agent_api_key`ã€`agent_api_base`
- [ ] T014 [US1] æ›´æ–° backend/app/services/agent_service.pyï¼šä¿®æ”¹ `is_available` å±æ€§ä½¿ç”¨ `settings.is_configured`
- [ ] T015 [US1] æ›´æ–°é”™è¯¯æ¶ˆæ¯ï¼šå°† "è¯·è®¾ç½® AGENT_API_KEY" æ”¹ä¸º "è¯·è®¾ç½® LLM_API_KEY"

**Checkpoint**: æ­¤æ—¶ Anthropic æ¨¡å¼åº”å®Œå…¨å¯ç”¨ï¼ŒSQL ç”Ÿæˆå’Œ Agent æ¨¡å¼å‡ä½¿ç”¨ç»Ÿä¸€é…ç½®

---

## Phase 4: User Story 2 - ä½¿ç”¨ OpenAI æ ¼å¼ API + ä»£ç†è½¬æ¢ (Priority: P2)

**Goal**: é€šè¿‡ claude-code-proxy æ”¯æŒ OpenAI å…¼å®¹æœåŠ¡

**Independent Test**: è®¾ç½® `LLM_API_TYPE=openai`ï¼Œå¯åŠ¨ Docker Composeï¼ˆå«ä»£ç†ï¼‰ï¼ŒéªŒè¯è¯·æ±‚æ­£ç¡®è½¬æ¢

### Implementation for User Story 2

- [ ] T016 [US2] ä¿®æ”¹ docker-compose.ymlï¼šæ·»åŠ  claude-code-proxy æœåŠ¡é…ç½®ï¼ˆä½¿ç”¨ profiles: ["openai"]ï¼‰
- [ ] T017 [US2] é…ç½® claude-code-proxy æœåŠ¡ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ `OPENAI_API_KEY`ã€`PREFERRED_PROVIDER`ã€`BIG_MODEL`
- [ ] T018 [US2] é…ç½® claude-code-proxy æœåŠ¡ï¼šæ·»åŠ å¥åº·æ£€æŸ¥å’Œæ­£ç¡®çš„ç½‘ç»œé…ç½®
- [ ] T019 [US2] ä¿®æ”¹ backend æœåŠ¡ä¾èµ–ï¼šæ·»åŠ å¯¹ proxy æœåŠ¡çš„å¯é€‰ä¾èµ–ï¼ˆå½“ profile=openai æ—¶ï¼‰
- [ ] T020 [US2] æ›´æ–° backend/app/config.pyï¼šå½“ `llm_api_type=openai` æ—¶ï¼Œ`effective_api_base` é»˜è®¤è¿”å› `http://proxy:8082`

**Checkpoint**: æ­¤æ—¶ OpenAI æ¨¡å¼åº”å¯ç”¨ï¼Œå¯é€šè¿‡ `docker compose --profile openai up` å¯åŠ¨

---

## Phase 5: User Story 3 - é…ç½®éªŒè¯ä¸é”™è¯¯æç¤º (Priority: P3)

**Goal**: æä¾›æ¸…æ™°çš„é…ç½®éªŒè¯å’Œé”™è¯¯æç¤º

**Independent Test**: æ•…æ„é…ç½®é”™è¯¯çš„ API ç±»å‹ï¼ŒéªŒè¯ç³»ç»Ÿè¿”å›æ˜ç¡®çš„é”™è¯¯ä¿¡æ¯

### Implementation for User Story 3

- [ ] T021 [US3] åœ¨ backend/app/config.py æ·»åŠ  `validate_config()` å‡½æ•°ï¼Œæ£€æŸ¥é…ç½®å®Œæ•´æ€§
- [ ] T022 [US3] åœ¨ backend/app/main.py å¯åŠ¨æ—¶è°ƒç”¨é…ç½®éªŒè¯ï¼Œæä¾›æ¸…æ™°é”™è¯¯æç¤º
- [ ] T023 [US3] å½“ `LLM_API_TYPE=openai` ä½†ä»£ç†ä¸å¯è¾¾æ—¶ï¼Œæä¾›æ˜ç¡®é”™è¯¯æ¶ˆæ¯
- [ ] T024 [US3] å½“ `LLM_API_TYPE` å€¼æ— æ•ˆæ—¶ï¼ŒPydantic éªŒè¯åº”æä¾›æ˜ç¡®é”™è¯¯

**Checkpoint**: æ‰€æœ‰é…ç½®é”™è¯¯åœºæ™¯åº”æœ‰æ¸…æ™°æç¤º

---

## Phase 6: Polish & Cross-Cutting Concerns

**Purpose**: æ–‡æ¡£æ›´æ–°å’Œæœ€ç»ˆéªŒè¯

- [ ] T025 [P] æ›´æ–° README.mdï¼šæ·»åŠ æ–°ç¯å¢ƒå˜é‡è¯´æ˜
- [ ] T026 [P] æ›´æ–° QUICKSTART.mdï¼šæ·»åŠ  Anthropic/OpenAI æ¨¡å¼é…ç½®è¯´æ˜
- [ ] T027 [P] åˆ›å»º backend/tests/test_config.pyï¼šæµ‹è¯•é…ç½®ä¼˜å…ˆçº§å’Œå‘åå…¼å®¹æ€§
- [ ] T028 è¿è¡Œ quickstart.md ä¸­çš„éªŒè¯æ¸…å•
- [ ] T029 æ¸…ç†åºŸå¼ƒä»£ç ï¼šç§»é™¤ä¸å†éœ€è¦çš„ OpenAI SDK å¯¼å…¥ï¼ˆå¦‚æœå…¨éƒ¨è¿ç§»å®Œæˆï¼‰

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: æ— ä¾èµ– - å¯ç«‹å³å¼€å§‹
- **Foundational (Phase 2)**: ä¾èµ– Setup å®Œæˆ - **é˜»å¡æ‰€æœ‰ User Story**
- **User Story 1 (Phase 3)**: ä¾èµ– Foundational å®Œæˆ - MVP æ ¸å¿ƒ
- **User Story 2 (Phase 4)**: ä¾èµ– Foundational å®Œæˆ - å¯ä¸ US1 å¹¶è¡Œ
- **User Story 3 (Phase 5)**: ä¾èµ– US1 å®Œæˆï¼ˆéœ€è¦ç»Ÿä¸€é…ç½®ç”Ÿæ•ˆï¼‰
- **Polish (Phase 6)**: ä¾èµ–æ‰€æœ‰ User Story å®Œæˆ

### User Story Dependencies

```
Setup (Phase 1)
     â”‚
     â–¼
Foundational (Phase 2) â”€â”€â”€ BLOCKS ALL â”€â”€â”€â”
     â”‚                                    â”‚
     â–¼                                    â–¼
User Story 1 (P1)              User Story 2 (P2)
     â”‚                                    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         User Story 3 (P3)
                â”‚
                â–¼
         Polish (Phase 6)
```

### Within Each User Story

- é…ç½®å˜æ›´ â†’ æœåŠ¡å±‚å˜æ›´ â†’ é”™è¯¯å¤„ç†
- æ ¸å¿ƒåŠŸèƒ½ â†’ é›†æˆæµ‹è¯•

### Parallel Opportunities

- T001, T002 å¯å¹¶è¡Œï¼ˆSetup é˜¶æ®µï¼‰
- T008-T012 å¿…é¡»æŒ‰é¡ºåºï¼ˆåŒä¸€æ–‡ä»¶ llm_service.pyï¼‰
- T013-T015 å¯ä¸ T008-T012 å¹¶è¡Œï¼ˆä¸åŒæ–‡ä»¶ï¼‰
- T016-T020 å¯ä¸ US1 å¹¶è¡Œï¼ˆä¸åŒå…³æ³¨ç‚¹ï¼‰
- T025, T026, T027 å¯å¹¶è¡Œï¼ˆä¸åŒæ–‡ä»¶ï¼‰

---

## Parallel Example: User Story 1

```bash
# å¹¶è¡Œæ‰§è¡Œä¸åŒæ–‡ä»¶çš„ä»»åŠ¡:
Task: T013 "é‡æ„ agent_service.py ä½¿ç”¨ç»Ÿä¸€é…ç½®"
Task: T008 "é‡æ„ llm_service.py æ›¿æ¢ä¸º Anthropic Client"  # éœ€æŒ‰é¡ºåºå®Œæˆ T008-T012

# å¹¶è¡Œæ‰§è¡Œ Polish é˜¶æ®µ:
Task: T025 "æ›´æ–° README.md"
Task: T026 "æ›´æ–° QUICKSTART.md"
Task: T027 "åˆ›å»ºé…ç½®æµ‹è¯•"
```

---

## Implementation Strategy

### MVP First (User Story 1 Only)

1. Complete Phase 1: Setup
2. Complete Phase 2: Foundational (CRITICAL - ç»Ÿä¸€é…ç½®æ¨¡å‹)
3. Complete Phase 3: User Story 1 (Anthropic æ¨¡å¼)
4. **STOP and VALIDATE**: æµ‹è¯• SQL ç”Ÿæˆå’Œ Agent æ¨¡å¼
5. å¯éƒ¨ç½²/æ¼”ç¤º MVP

### Incremental Delivery

1. Setup + Foundational â†’ é…ç½®æ¨¡å‹å°±ç»ª
2. Add User Story 1 â†’ Anthropic æ¨¡å¼å¯ç”¨ â†’ **MVP!**
3. Add User Story 2 â†’ OpenAI æ¨¡å¼å¯ç”¨ï¼ˆéœ€ä»£ç†ï¼‰
4. Add User Story 3 â†’ é”™è¯¯æç¤ºå®Œå–„
5. æ¯ä¸ª Story ç‹¬ç«‹å¢åŠ ä»·å€¼

---

## Key Implementation Notes

### llm_service.py é‡æ„è¦ç‚¹

1. **å¯¼å…¥å˜æ›´**:
   ```python
   # æ—§
   from openai import OpenAI
   # æ–°
   from anthropic import Anthropic
   ```

2. **Client åˆå§‹åŒ–**:
   ```python
   # æ—§
   self._client = OpenAI(api_key=..., base_url=...)
   # æ–°
   self._client = Anthropic(api_key=..., base_url=...)
   ```

3. **API è°ƒç”¨å˜æ›´**:
   ```python
   # æ—§
   response = self.client.chat.completions.create(
       model=settings.llm_model,
       messages=[{"role": "system", "content": ...}, {"role": "user", "content": ...}],
   )
   content = response.choices[0].message.content
   
   # æ–°
   response = self.client.messages.create(
       model=settings.effective_model,
       system=system_prompt,  # system ç‹¬ç«‹å‚æ•°
       messages=[{"role": "user", "content": user_prompt}],
       max_tokens=4096,
   )
   content = response.content[0].text
   ```

### å‘åå…¼å®¹éªŒè¯ç‚¹

- `AGENT_API_KEY` è®¾ç½®ååº”ç­‰åŒäº `LLM_API_KEY`
- ç°æœ‰ `.env` é…ç½®æ— éœ€ä¿®æ”¹å³å¯å·¥ä½œ
- æ—§å˜é‡ä¼˜å…ˆçº§ä½äºæ–°å˜é‡

---

## Notes

- [P] tasks = ä¸åŒæ–‡ä»¶ï¼Œæ— ä¾èµ–
- [Story] æ ‡ç­¾å°†ä»»åŠ¡æ˜ å°„åˆ°ç‰¹å®š User Story
- æ¯ä¸ª User Story åº”å¯ç‹¬ç«‹å®Œæˆå’Œæµ‹è¯•
- æ¯ä¸ªä»»åŠ¡æˆ–é€»è¾‘ç»„å®Œæˆåæäº¤
- å¯åœ¨ä»»ä½•æ£€æŸ¥ç‚¹åœæ­¢éªŒè¯ Story
- é¿å…ï¼šæ¨¡ç³Šä»»åŠ¡ã€åŒæ–‡ä»¶å†²çªã€ç ´åç‹¬ç«‹æ€§çš„è·¨ Story ä¾èµ–

